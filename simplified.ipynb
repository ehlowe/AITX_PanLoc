{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import custom_model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((2048, 1024)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Get the data\n",
    "all_data = []\n",
    "# folder_paths=[r\"D:\\Unity\\AITX_PanLoc\\Assets\\data\\21.34938_-12.25565_folder\"]\n",
    "# folder_paths=[r\"D:\\Unity\\AITX_PanLoc\\Assets\\data\\-30.19358_-31.25824_folder\"]\n",
    "folder_paths=[r\"D:\\Unity\\AITX_PanLoc\\Assets\\data\\25.53818_-38.33201_folder\"]\n",
    "for folder_path in folder_paths:\n",
    "    file_paths = os.listdir(folder_path)\n",
    "\n",
    "    base_image = next((os.path.join(folder_path, f) for f in file_paths if \"_1.\" in f and f.endswith(\".jpg\")), None)\n",
    "    base_text = next((os.path.join(folder_path, f) for f in file_paths if \"_1.\" in f and f.endswith(\".txt\")), None)\n",
    "\n",
    "    if not (base_image and base_text):\n",
    "        print(f\"Base image or text not found in {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(base_text, 'r') as f:\n",
    "        base_position = [float(x) for x in f.read().split(\"(\")[1].split(\")\")[0].split(\",\")]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if \"_1.\" in file_path or not file_path.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(folder_path, file_path)\n",
    "        txt_path = os.path.join(folder_path, f\"{os.path.splitext(file_path)[0]}.txt\")\n",
    "\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, 'r') as f:\n",
    "                location = [float(x) for x in f.read().split(\"(\")[1].split(\")\")[0].split(\",\")]\n",
    "            relative_x = location[0] - base_position[0]\n",
    "            relative_y = location[2] - base_position[2]\n",
    "            \n",
    "            max_distance = 50\n",
    "            normalized_x = np.clip(relative_x / max_distance, -1, 1)\n",
    "            normalized_y = np.clip(relative_y / max_distance, -1, 1)\n",
    "            \n",
    "            all_data.append((base_image, image_path, normalized_x, normalized_y, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the paths into tensors of the image\n",
    "for i, data in enumerate(all_data):\n",
    "    base_image=Image.open(data[0])\n",
    "    base_image_tensor=transform(base_image).unsqueeze(0)\n",
    "    current_image=Image.open(data[1])\n",
    "    current_image_tensor=transform(current_image).unsqueeze(0)\n",
    "    position=(data[2], data[3])\n",
    "    all_data[i]=(base_image_tensor, current_image_tensor, position)\n",
    "    base_image.close()\n",
    "    current_image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in all_data[:15]:\n",
    "    print(data[2])\n",
    "\n",
    "    # dataset = ImagePairDataset(all_data)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_img, current_img, position = self.data[idx]\n",
    "        return base_img.squeeze(0), current_img.squeeze(0), torch.tensor(position)\n",
    "\n",
    "# split into training and validation by 80/20\n",
    "split = int(0.8 * len(all_data))\n",
    "train_data = all_data[:split]\n",
    "val_data = all_data[split:]\n",
    "\n",
    "train_dataset = ImagePairDataset(train_data)\n",
    "val_dataset = ImagePairDataset(val_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo model weights\n",
    "model = custom_model.ImagePositionPredictor()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "def custom_loss(pred, target):\n",
    "    # convert everything to float\n",
    "    pred = pred.float()\n",
    "    target = target.float()\n",
    "    # print(pred[:, :2])\n",
    "    # print(target[:, :2])\n",
    "    # print(F.mse_loss(pred[:, :2], target[:, :2]))\n",
    "\n",
    "    # return mse_loss#+bce_loss\n",
    "\n",
    "    #mse_loss = F.mse_loss(pred[:, :2], target[:, :2])\n",
    "    bce_loss = F.l1_loss(pred[:, :2], target[:, :2])\n",
    "    return bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3471\n",
      "Validation Loss: 0.3364\n",
      "Epoch 2/10, Loss: 0.3332\n",
      "Validation Loss: 0.3327\n",
      "Epoch 3/10, Loss: 0.3205\n",
      "Validation Loss: 0.3494\n",
      "Epoch 4/10, Loss: 0.3353\n",
      "Validation Loss: 0.3369\n",
      "Epoch 5/10, Loss: 0.3359\n",
      "Validation Loss: 0.3229\n",
      "Epoch 6/10, Loss: 0.3218\n",
      "Validation Loss: 0.3450\n",
      "Epoch 7/10, Loss: 0.3356\n",
      "Validation Loss: 0.3474\n",
      "Epoch 8/10, Loss: 0.3351\n",
      "Validation Loss: 0.3276\n",
      "Epoch 9/10, Loss: 0.3188\n",
      "Validation Loss: 0.3249\n",
      "Epoch 10/10, Loss: 0.3162\n",
      "Validation Loss: 0.3239\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best=10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for base_img, current_img, position in train_dataloader:\n",
    "        base_img, current_img, position = base_img.to(device), current_img.to(device), position.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(base_img, current_img)\n",
    "        loss = custom_loss(output, position)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for base_img, current_img, position in val_dataloader:\n",
    "            base_img, current_img, position = base_img.to(device), current_img.to(device), position.to(device)\n",
    "\n",
    "            output = model(base_img, current_img)\n",
    "            loss = custom_loss(output, position)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Validation Loss: {total_loss / len(val_dataloader):.4f}\")\n",
    "\n",
    "    if (total_loss / len(val_dataloader))<best:\n",
    "        best=total_loss / len(val_dataloader)\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "    scheduler.step(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for base_img, current_img, position in val_dataloader:\n",
    "        base_img, current_img, position = base_img.to(device), current_img.to(device), position.to(device)\n",
    "\n",
    "        output = model(base_img, current_img)\n",
    "        loss = custom_loss(output, position)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "print(f\"Validation Loss: {total_loss / len(val_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for base_img, current_img, position in val_dataloader:\n",
    "        base_img, current_img, position = base_img.to(device), current_img.to(device), position.to(device)\n",
    "\n",
    "        numbers = [tensor.item() for tensor in output[0]]\n",
    "        print(numbers[0:2])\n",
    "        numbers = [tensor.item() for tensor in position[0]]\n",
    "        print(numbers)\n",
    "        print()\n",
    "\n",
    "        output = model(base_img, current_img)\n",
    "        loss = custom_loss(output, position)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediciton on a data point\n",
    "model.eval()\n",
    "\n",
    "base_img, current_img, position = dataset[15]\n",
    "\n",
    "base_img = base_img.to(device).unsqueeze(0)\n",
    "\n",
    "current_img = current_img.to(device).unsqueeze(0)\n",
    "\n",
    "output = model(base_img, current_img)\n",
    "\n",
    "print(f\"Predicted: {output}, Actual: {position}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePositionPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImagePositionPredictor, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for each input image\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512, 256)  # 512 comes from 256 * 2 (two images)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # Output: x, y, confidence\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        # Process first image\n",
    "        x1 = self._process_single_image(img1)\n",
    "        \n",
    "        # Process second image\n",
    "        x2 = self._process_single_image(img2)\n",
    "        \n",
    "        # Concatenate features from both images\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Apply tanh to first two outputs (x, y) to constrain them between -1 and 1\n",
    "        # Apply sigmoid to the third output (confidence) to constrain it between 0 and 1\n",
    "        return torch.cat((torch.tanh(x[:, :2]), torch.sigmoid(x[:, 2].unsqueeze(1))), dim=1)\n",
    "    \n",
    "    def _process_single_image(self, img):\n",
    "        x = F.relu(self.bn1(self.conv1(img)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.gap(x)\n",
    "        return torch.flatten(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total parameters\n",
    "model = ImagePositionPredictor()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = ImagePositionPredictor()\n",
    "img1 = torch.randn(1, 3, 224, 224)  # Batch size 1, 3 color channels, 224x224 resolution\n",
    "img2 = torch.randn(1, 3, 224, 224)\n",
    "output = model(img1, img2)\n",
    "print(output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths=[r\"D:\\Unity\\AITX_PanLoc\\Assets\\data\\21.34938_-12.25565_folder\"]\n",
    "\n",
    "all_data=[]\n",
    "for folder_path in folder_paths:\n",
    "    # Get all files in folder\n",
    "    image_paths_and_locations={}\n",
    "    file_paths=os.listdir(folder_path)\n",
    "\n",
    "    # Get the Base Image\n",
    "    base_image=None\n",
    "    base_position=None\n",
    "    base_text_path=None\n",
    "    for file_path in file_paths:\n",
    "        if \"_1.\" in file_path:\n",
    "            if file_path.endswith(\".jpg\"):\n",
    "                base_image = file_path\n",
    "            \n",
    "            # if txt set the location\n",
    "            if file_path.endswith(\".txt\"):\n",
    "                base_text_path=file_path\n",
    "                text_data=open(os.path.join(folder_path, file_path)).read().split(\"\\n\")\n",
    "                base_position=[float(x) for x in text_data[0].split(\"(\")[1].split(\")\")[0].split(\",\")]\n",
    "\n",
    "    if base_image and base_position:\n",
    "        print(\"Found base image and position\")\n",
    "    else:\n",
    "        print(\"Base image and position not found\")\n",
    "\n",
    "\n",
    "    # Get the input images\n",
    "    files_paths_input_data=deepcopy(file_paths)\n",
    "    files_paths_input_data.remove(base_image)\n",
    "    files_paths_input_data.remove(base_text_path)\n",
    "\n",
    "    image_paths_and_locations={}\n",
    "    for file_path in files_paths_input_data:\n",
    "        if \"_1.\" in file_path:\n",
    "            continue\n",
    "\n",
    "        # Get file name and initialize the data\n",
    "        file_name=file_path.split(\".\")[0]\n",
    "        if image_paths_and_locations.get(file_name, None)==None:\n",
    "                image_paths_and_locations[file_name]={}\n",
    "\n",
    "        # if image set the image path\n",
    "        if file_path.endswith(\".jpg\"):\n",
    "            image_paths_and_locations[file_name][\"image\"]=os.path.join(folder_path, file_path)\n",
    "\n",
    "        # if txt set the location\n",
    "        if file_path.endswith(\".txt\"):\n",
    "            text_data=open(os.path.join(folder_path, file_path)).read().split(\"\\n\")\n",
    "            image_paths_and_locations[file_name][\"location\"]=[float(x) for x in text_data[0].split(\"(\")[1].split(\")\")[0].split(\",\")]\n",
    "\n",
    "    all_data+=list(image_paths_and_locations.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_and_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, folder_paths, transform=None):\n",
    "        self.folder_paths = folder_paths\n",
    "        self.transform = transform\n",
    "        self.image_pairs = self._load_image_pairs()\n",
    "\n",
    "    def _load_image_pairs(self):\n",
    "        all_data = []\n",
    "        for folder_path in self.folder_paths:\n",
    "            file_paths = os.listdir(folder_path)\n",
    "\n",
    "            # Get the Base Image\n",
    "            base_image = None\n",
    "            base_position = None\n",
    "            for file_path in file_paths:\n",
    "                if \"_1.\" in file_path:\n",
    "                    if file_path.endswith(\".jpg\"):\n",
    "                        base_image = os.path.join(folder_path, file_path)\n",
    "                    if file_path.endswith(\".txt\"):\n",
    "                        base_text_path = os.path.join(folder_path, file_path)\n",
    "                        with open(base_text_path, 'r') as f:\n",
    "                            text_data = f.read().split(\"\\n\")\n",
    "                        base_position = [float(x) for x in text_data[0].split(\"(\")[1].split(\")\")[0].split(\",\")]\n",
    "\n",
    "            if not (base_image and base_position):\n",
    "                print(f\"Base image and position not found in {folder_path}\")\n",
    "                continue\n",
    "\n",
    "            # Get the input images\n",
    "            image_pairs = []\n",
    "            for file_path in file_paths:\n",
    "                if \"_1.\" in file_path or not file_path.endswith(\".jpg\"):\n",
    "                    continue\n",
    "\n",
    "                file_name = file_path.split(\".\")[0]\n",
    "                image_path = os.path.join(folder_path, file_path)\n",
    "                txt_path = os.path.join(folder_path, f\"{file_name}.txt\")\n",
    "\n",
    "                if os.path.exists(txt_path):\n",
    "                    with open(txt_path, 'r') as f:\n",
    "                        text_data = f.read().split(\"\\n\")\n",
    "                    location = [float(x) for x in text_data[0].split(\"(\")[1].split(\")\")[0].split(\",\")]\n",
    "                    \n",
    "                    # Calculate relative position\n",
    "                    relative_x = location[0] - base_position[0]\n",
    "                    relative_y = location[2] - base_position[2]\n",
    "                    \n",
    "                    # Normalize to [-1, 1] range (you may need to adjust this based on your data)\n",
    "                    max_distance = 0.1  # Adjust this value based on your data's scale\n",
    "                    normalized_x = np.clip(relative_x / max_distance, -1, 1)\n",
    "                    normalized_y = np.clip(relative_y / max_distance, -1, 1)\n",
    "                    \n",
    "                    # Add confidence (you may want to adjust this based on your needs)\n",
    "                    confidence = 1.0\n",
    "\n",
    "                    image_pairs.append((base_image, image_path, normalized_x, normalized_y, confidence))\n",
    "\n",
    "            all_data.extend(image_pairs)\n",
    "\n",
    "        return all_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            base_img_path, img_path, x, y, confidence = self.image_pairs[idx]\n",
    "            base_img = Image.open(base_img_path).convert('RGB')\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                base_img = self.transform(base_img)\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return base_img, img, torch.tensor([x, y, confidence], dtype=torch.float)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image pair at index {idx}: {str(e)}\")\n",
    "            # Return a dummy sample\n",
    "            dummy_img = torch.zeros((3, 2048, 1024))\n",
    "            dummy_label = torch.tensor([0, 0, 0], dtype=torch.float)\n",
    "            return dummy_img, dummy_img, dummy_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for img1, img2, labels in train_loader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img1, img2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * img1.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img1, img2, labels in val_loader:\n",
    "                img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "                outputs = model(img1, img2)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * img1.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set up data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((2048, 1024)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "folder_paths = [r\"D:\\Unity\\AITX_PanLoc\\Assets\\data\\21.34938_-12.25565_folder\"]\n",
    "train_dataset = ImagePairDataset(folder_paths=folder_paths, transform=transform)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(train_dataset.image_pairs))\n",
    "val_size = len(train_dataset.image_pairs) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImagePositionPredictor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImagePositionPredictor().to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "trained_model = train_model(model, train_loader, val_loader, num_epochs, learning_rate, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'image_position_predictor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img1, img2, labels in train_loader:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

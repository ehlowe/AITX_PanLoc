{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dataset import ImagePairDataset\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your ImagePositionPredictor model here\n",
    "class ImagePositionPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImagePositionPredictor, self).__init__()\n",
    "        \n",
    "        # Convolutional layers for each input image\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512, 256)  # 512 comes from 256 * 2 (two images)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # Output: x, y, confidence\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        # Process first image\n",
    "        x1 = self._process_single_image(img1)\n",
    "        \n",
    "        # Process second image\n",
    "        x2 = self._process_single_image(img2)\n",
    "        \n",
    "        # Concatenate features from both images\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Apply tanh to first two outputs (x, y) to constrain them between -1 and 1\n",
    "        # Apply sigmoid to the third output (confidence) to constrain it between 0 and 1\n",
    "        return torch.cat((torch.tanh(x[:, :2]), torch.sigmoid(x[:, 2].unsqueeze(1))), dim=1)\n",
    "    \n",
    "    def _process_single_image(self, img):\n",
    "        x = F.relu(self.bn1(self.conv1(img)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.gap(x)\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for base_img, img, labels in train_loader.dataset:\n",
    "            try:\n",
    "                base_img, img, labels = base_img.to(device), img.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(base_img, img)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * base_img.size(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for base_img, img, labels in val_loader:\n",
    "                try:\n",
    "                    base_img, img, labels = base_img.to(device), img.to(device), labels.to(device)\n",
    "                    outputs = model(base_img, img)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * base_img.size(0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during validation: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((2048, 1024)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = ImagePairDataset(folder_paths=folder_paths, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.image_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item 1 in dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=multiprocessing.cpu_count())\n",
    "\n",
    "print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set up data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((2048, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    folder_paths = [r\"D:\\Unity\\AITX_PanLoc\\Assets\\data\\21.34938_-12.25565_folder\"]\n",
    "    full_dataset = ImagePairDataset(folder_paths=folder_paths, transform=transform)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # Use multiple workers for data loading\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Initialize the model and move it to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImagePositionPredictor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for img1, img2, labels in train_loader.dataset:\n",
    "        print(img1.shape, img2.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (base_img, img, labels) in enumerate(train_loader):\n",
    "    print(batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Initialize the model and move it to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImagePositionPredictor().to(device)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs, learning_rate, device)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(trained_model.state_dict(), 'image_position_predictor.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
